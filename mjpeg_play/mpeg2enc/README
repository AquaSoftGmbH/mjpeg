mpeg2enc encodes MJPEG AVIs, Quicktime Files or Edit List Files
to MPEG-1/2 video streams.

DISCLAIMER:

This software is from the MPEG Software Simulation Group.
See notices in source code regarding Patents and miscellaneous
IP issues.

POST  MPEG SSG Development

Rainer Johanni changed the input routine so that "lavtools" Edit
Lists, AVI's and Quicktime input is accepted, introduced command line
params instead of a param file and made some simple optimizations.
Rainer also did the initial mod to accept the new special yuv format.

Andrew Stevens added more intelligent motion compensation and
optimised some key calculations that run badly on modern highly
pipelined machines.  These were then further refined based on the
results of some work by Juan Remero Arroyo (who would have guessed 4*4
blocks still produce useful results!).
 
Chris Atenasio's contributed MMX2/SSE code for the motion compensation
macroblock group difference calculation.  

Andrew Stevens added a lot more MMX1 code for motion compensation and
adapted Chris' code for his intelligent motion compensation mods and
ported the fdct/idct MMX code to nasm.

Andrew Stevens added some simple pre-processing filters to improve
output quality when using noisy (e.g. broadcast) source video.

Andrew Stevens modified the bit-allocation routines to use a 
more accurate method of estimating bits needed for good quality results.

Andrew Stevens fixed a number of overflow and calculation errors
associated with large files.

Andrew Stevens ported in and tidied up a number of
MMX routines from the bbmpeg project for windows (see
www.delphi.com/bbmpeg_ug) to accelerate a few "second line" but
nonetheless significantly time-consuming functions.

Andrew Stevens added a new much faster MMX based quantization routine.

In case you're wondering: the upshot of this disclaimer is that the
functionality is exactly as per the original but the software now runs
around 12 times as fast with sensible (search radius 15) motion compensation
settings, and you can now get half-decent looking results when working with
typical broadcast quality sources.

USAGE:

Output file name (-o option) is required!
Usage: ./mpeg2enc [params] inputfiles
   where possible params are:
   -m num     MPEG level (1 or 2) default: 1
   -b num     Bitrate in KBit/sec (default: 1152 KBit/s)
   -q num     Quality factor [1..31] (1 is best, no default)
	      This option selects a simple form of variable-bit-rate encoding
	      -q and -b are mutually exclusive.
   -o name    Outputfile name (REQUIRED!!!)
   -r num     Search radius [0..32] (default 0: don't search at all)
   -s num     Special output format option:
                 0 output like input, nothing special
                 1 create half height/width output from interlaced input
                 2 create 480 wide output from 720 wide input (for SVCD)
   -d num     Drop lsbs of samples [0..3] (default: 0)
   -n num     Noise filter (low-pass) [0..2] (default: 0) Bigger numbers stronger smoothing.
   -f num     Fraction of fast motion estimates to consider in detail (1/num) [2..20] (default: 10)
   -t         Activate dynamic thresholding of motion compensation window size 


FAST MOTION COMPENSATION:

For MPEG to achieve good quality results it has to find a good match
between each 8*8 "macro-block" of pixels in the current frame and
an 8*8 region in a preceding or following frame.  The better the match
the less information it has to drop when compressing.  Obviously, finding
such a good match in *moving* pictures requires a lot of searching.

To do this (reasonably) quickly we use a trick: we first search
coarsely first comparing sums of 4*4 pixel groups and then 2*2 pixel
groups rather than individual pixels. Technically: we do an initial
search using sub-sampled data and use this to choose the places to
search for detail single pixel and sub-pixel matches.
This reduces the amount of search *dramatically*. 

Since a good exact matches (down to half-pixels) will also give a
good 2*2 match we know the best match should be in amoungst the best
2*2 matches.  However, there may be some false positives.  So we
can't just work with the very best.  Instead we choose some fraction
(the default is 1/10) and look for the best exact match corresponding
to these good 2*2 matches.  Experiments and a bit of simple
statistics revealed that it was exceedingly rare to miss the best
match doing this, and even when we did miss the match found was
*almost* as good.


Obviously, your mileage may vary so you may want to fiddle with the -f
setting if you're getting rotten results.  Contact
Andrew.Stevens@comlab.ox.ac.uk if you think you have found a case
where the fast compensation fails!

In case you're wondering: the performance gain is a factor of around 2-3.

SEARCH THRESHOLDING

Its a pain to watch the encoder grinding its way through huge motion
compensation search radii that are really only needed on the fastest
moving of scenes.  As a way around this the encoder now supports
thresholding of  motion compensation search.  It first searches within
a small radius (half the specified).  If this already yields a result
that is better than average (in terms of absolute difference) it aborts
the search there.  Only if the best match found initially is poor does
the program bother searching more widely.  The net result is motion
compensation is speeded by another factor of two for near zero quality
loss.

UPDATE: Actually, this currently doesn't happen instead the thresholding
takes place as 2*2 pel matches are evaluated in order of closeness.  It may 
be re-instated if the performance gains look worthwhile.
N.b. because it is possible (if very unlikely) that non-trivial quality
losses may occur (and the gains in terms of total run-time
are not huge) thresholding defaults *off*.  Use "-t" to turn it on.

As an aside to prevent short patches of very rapidly changing scenes
with poor matching after motion compensation making the encoder "lazy"
by skewing the average, the system uses *two* averages on long-term
one short-term.  It uses the *smaller* of the two when deciding whether
or not to bother searching further.   


NOISE FILTERING and BIT DROPPING

MPEG compression assumes that the pictures in a video sequence are
closely correlated (once you've applied motion compensatino to search
for good matches).  Unfortunately, if you have a noisy original video
source this assumption isn't really true.  The noise introduces lots
of spurious differences betwen actually very similar pictures.  The
end result is ugly "artifacted" results where MPEG has dropped real
information in favour of spurious noise.  The solution is to try to
filter the noise out of the source.   Currently, nothing terribly
clever is done: just simple smoothing. -n 1 smooths gently and is almost
imperceptible, -n 2 is rather stronger and is noticeable if you compare
source video.  -n3 is really brutal and probably only sensible for material
captured at high resolution. However, if you really have noisy sources then -n 2 
or -n 3 is infinitely preferable to the swimming sea of artefacts MPEG otherwise
generates.

SMARTER BIT ALLOCATION (A.Stevens Aug 2000)

The original source code makes heavy use of the variance of
blocks as a measure of their information content.  This is actually
pretty poor as what really counts is how many bits it will take
to encoded the quantised DCT of the block without excessive losses.
The bit allocation strategy is also very naive, essentially the
encoder (more or less) tries to keep the bits allocated to each
block the same.   This is bad news if you have a picture where
a lot of the information content comes low down....

As a first step to better quality encoding the current source now
uses the sum of absolute block coefficients as a measure of information
content. Bits are then allocated on the basis of how much information
is being covered so that bits are held back if high information
material comes late in the frame.

These modifications appear to give noticeable better results, greatly
reducing fringes around sharp contrasts (e.g. titles).  

Update: the new metrics still need a little more tuning. Watch this space...

PERFORMANCE: MMX/MMX2/SSE (A.Stevens Jul/Aug 2000)

In addition to improved fast motion compensation the updated code
also has x86 assembly language MMX/SSE routines for key motion
compensation calculations (8*8 and 16*16 block absolute difference sums).

Unfortunately, the original MMX (MMX1) instruction set is somewhat
deficient in a couple of key areas.  In particular there's no way of
preserving carries. In this case: computing a 16 bit sum of packed 8
bit unsigned differences or averaging 8 bit values are greatly
complicated by this.  Thus the performance gains from moving to MMX aren't as
spectacular as one might expect: only around 50% speed up.


Fortunately, MMX2 as found on Athlon's and Intel chips with SSE adds
some *really* handy instructions.  Yep, you guessed it, an instruction
for computing a sum of absolute differences of 8-bit packed data.  They
might as well have gone the whole hog and called it "MPEGMOTIONCOMP"...
Chris Atenasio's code works like lightning and was also adapted for
fast motion compensation search by yours truly.  Speed up is better than
100% (more than twice as fast).  Its even a bit quicker now due
to some tuning by yours truly.  Unfortunately, the current auto-detection code
probably doesn't select the right MMX2 routines on Athlon's.   This should be
fixed soon...

There have also been some gross-me-out hacky optimisations made
to the non-MMX code to make it run better on modern pipelined CPU's.
However, if you're trying to compress long MPEG sequences on a pre-MMX
x86 CPU you *will* need some serious patience ;-).  I reckon compressing
a 1 hour TV show is now tolerable with the new super-duper fast
mpeg2enc and a  300Mhz+  CPU on the job.  Personally, I split the
work between 4 CPU's: Celeron 366, 2*PIII-450 and a K6-2 500.

I suspect the code would need serious reworking on an alpha etc as it
merrily abuses data alignment constraints.

STILL MORE PERFORMANCE?

Not easy to see how bit improvements can be made without radical
rebuilding of the code.  The program still spends around 50% of its
time in motion compesation search.  Further speed ups would require
more aggressive algoriths. A logarithmic search with row/column sums
might be worth investigating for a "quick and dirty" compression mode
with real-time pretensions.  However, to get the additional factor of
2-3 needed to allow real-time compression on a single modern GHz CPU
other parts of the program would need to be investigated too...

One possibility it sliding search windows based on previous motion
compensations to keep search *small*...


BETTER QUALITY?

- There is still a lot of room for improvements to bit-allocation.
The most interesting issue would be to find a reasonable quality
metric and allow constant quality (variable bit-rate) encoding.
The current "Quality factor"  method is just constant quantisation
which is only a coarse approximation to constant quality.  One
possibility would be to compare reconstructed blocks with the
originals and take a second pass, reducing quantisation on those
with unacceptably low SNR's.


- It would probably be smart to replace the current pre-processor
filtering with filtering based on modifications of the DCT quantisation
matrices.   Strong suppression of HF coefficients would work wonders
on noise I suspect.  Update: either I'm doing it wrong or it doesn't
work too well.  One problem is the lack of filtering messing up 
motion compensation.


INPUT FORMAT

YUV4MPEG\n
<horizonal size> <vertical size> <frame_rate_code>\n
[FRAME\n<Y-data><U-data><V-data> ...]





